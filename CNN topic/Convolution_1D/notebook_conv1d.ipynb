{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature as feat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths, target_length): \n",
    "    data = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading and preprocessing data\", unit=\"file\"):\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else: \n",
    "            audio = audio[:target_length]\n",
    "\n",
    "        audio = bandpass_filter(audio, sr)\n",
    "\n",
    "        data.append(audio)\n",
    "\n",
    "    print(\"Done\")\n",
    "    return np.array(data)\n",
    "\n",
    "def load_and_preprocess_data_with_spectrogram(file_paths, target_length):\n",
    "    data = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading and preprocessing data\", unit=\"file\"):\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        if len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "        else: \n",
    "            audio = audio[:target_length]\n",
    "\n",
    "        sos = signal.butter(6, [5000, 100000], 'bandpass', fs=sr, output='sos')\n",
    "        audio = signal.sosfiltfilt(sos, audio)\n",
    "\n",
    "        spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "        spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        # Normalize the spectrogram\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min())\n",
    "        data.append(spectrogram)\n",
    "\n",
    "    print(\"Done\")\n",
    "    return np.array(data)\n",
    "\n",
    "def bandpass_filter(audio, sr):\n",
    "    sos = signal.butter(6, [5000, 100000], 'bandpass', fs=sr, output='sos')\n",
    "    audio = signal.sosfiltfilt(sos, audio)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def build_model(target_length):\n",
    "    print(\"\\nCreating model\")\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(32, kernel_size=9, activation='relu', input_shape=(128, 101))) \n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # Second convolutional layer\n",
    "    model.add(layers.Conv1D(32, kernel_size=9, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    # Third convolutional layer\n",
    "    model.add(layers.Conv1D(32, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # Flatten the output for the fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    # First fully connected layer\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer='l2')) #! L2 regularization to remove after\n",
    "    # Second fully connected layer\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer='l2')) #! L2 regularization to remove after\n",
    "    # Dropout regularization to avoid overfitting\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # Binary classification output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd().parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ===== Set parameters ======\n",
    "grandparent_dir = Path.cwd().parents[1]\n",
    "conv1D_directory = Path.cwd()\n",
    "test_directory = grandparent_dir / \".dataset\" / \"X_test\"\n",
    "models_directory = conv1D_directory / \"models\"\n",
    "\n",
    "\n",
    "# Set the path to the downloaded data\n",
    "download_path = grandparent_dir / \".dataset\"\n",
    "\n",
    "# Audio parameters\n",
    "sample_rate = 256000\n",
    "audio_duration_seconds = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ====== Load and preprocess data ====== \n",
    "# Read labels file\n",
    "labels_file = download_path / \"Y_train_ofTdMHi.csv\"\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df[\"relative_path\"] = Path(download_path) / \"X_train\" / df[\"id\"]\n",
    "# df[\"relative_path\"] = str(download_path) + \"/X_train/\" + df[\"id\"]\n",
    "\n",
    "# Drop id column (replaced it with relative_path)\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "df.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "\n",
    "# invert relative_path and label columns positions\n",
    "df = df[[\"relative_path\", \"label\"]]\n",
    "print(f\"### There are {len(df)} audio files in the dataset.\")\n",
    "\n",
    "table = f\"\"\"\n",
    "Here is the split into good and bad signals:\n",
    "| Label   | Count   |\n",
    "|:-------:|:-------:|\n",
    "| 0       | {df['label'].value_counts()[0]:7} |\n",
    "| 1       | {df['label'].value_counts()[1]:7} |\"\"\"\n",
    "print(table, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preprocessing data\")\n",
    "target_length = int(sample_rate * audio_duration_seconds)\n",
    "X = load_and_preprocess_data_with_spectrogram(df[\"relative_path\"], target_length)\n",
    "y = df[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSplitting data into train and validation sets\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(target_length) # Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------ Training model ------------------\", end=\"\\n\\n\")\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final model accuracy\n",
    "print(\"\\n------------------ Model accuracy ------------------\")\n",
    "_, accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Accuracy: %.2f\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------ Saving model ------------------\", end=\"\\n\\n\")\n",
    "model_name = \"1d_cnn_l2_spectro.keras\"\n",
    "\n",
    "os.mkdir(Path(models_directory)) if not os.path.exists(Path(models_directory)) else None\n",
    "model.save(Path(models_directory) / model_name)\n",
    "print(f\"Model {model_name} saved at {models_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------ Plotting accuracy ------------------\", end=\"\\n\\n\")\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(folder_path, target_length):\n",
    "    file_paths = list(Path(folder_path).rglob('*.wav'))  # Assuming the audio files are in WAV format\n",
    "    return load_and_preprocess_data(file_paths, target_length)\n",
    "\n",
    "def load_test_data_with_spectrogram(folder_path, target_length):\n",
    "    file_paths = list(Path(folder_path).rglob('*.wav'))  # Assuming the audio files are in WAV format\n",
    "    return load_and_preprocess_data_with_spectrogram(file_paths, target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = load_test_data_with_spectrogram(test_directory, target_length)\n",
    "\n",
    "model_to_test = \"1d_cnn_l2_spectro.keras\"\n",
    "model = models.load_model(models_directory / model_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [file_path.name for file_path in Path(test_directory).rglob('*.wav')]\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------\")\n",
    "print(predictions.shape)\n",
    "print(len(file_names))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as csv for submission file\n",
    "submission_filename = \"submission.csv\"\n",
    "\n",
    "df = pd.DataFrame({'id': file_names, 'pos_label': predictions[:, 0]})\n",
    "df.to_csv(Path(conv1D_directory) / submission_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
