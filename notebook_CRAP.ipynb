{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T06:23:45.592188100Z",
     "start_time": "2023-10-20T06:23:45.573756200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T19:03:23.874859Z",
     "start_time": "2023-10-19T19:03:23.871858Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioUtil:\n",
    "    \"\"\"Static class for audio processing helper functions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(audio_file: str):\n",
    "        \"\"\"Load an audio file. Return the signal as a tensor and the sample rate\"\"\"\n",
    "        sig, sr = librosa.load(audio_file, sr=256000)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_duration(sig, sr):\n",
    "        \"\"\"Return the duration of an audio signal in seconds\"\"\"\n",
    "        return librosa.get_duration(sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectro_gram(sig: np.array, sr: int, n_mels=32, n_fft=1024):\n",
    "        \"\"\"Generate a Spectrogram\"\"\"\n",
    "        # get mel spectrogram\n",
    "        spec = librosa.feature.melspectrogram(y=sig, sr=sr)\n",
    "        spec = librosa.amplitude_to_db(spec)\n",
    "        return spec\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_specs_size(spec):\n",
    "        \"\"\"Return the size of a spectrogram image\"\"\"\n",
    "        return spec.shape\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_mel_spectro_gram(spec: np.array, sr: int):\n",
    "        \"\"\"Plot a Spectrogram\"\"\"\n",
    "        # plot mel spectrogram\n",
    "        fig, ax = plt.subplots()\n",
    "        S_dB = librosa.power_to_db(spec, ref=np.max)\n",
    "        img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                                y_axis='mel', sr=sr,\n",
    "                                ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T19:03:52.560359Z",
     "start_time": "2023-10-19T19:03:52.535360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ** 23168 ** audio files in the dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       relative_path  label\n",
       "0  c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...    0.0\n",
       "1  c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...    1.0\n",
       "2  c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...    1.0\n",
       "3  c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...    1.0\n",
       "4  c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_cl...    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_path = Path.cwd() / \".dataset\"\n",
    "\n",
    "# Read metadata file\n",
    "labels_file = download_path / \"Y_train_ofTdMHi.csv\"\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating fold and file name\n",
    "df[\"relative_path\"] = str(download_path) + \"/X_train/\" + df[\"id\"]\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "df.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "# invert relative_path and label columns positions\n",
    "df = df[[\"relative_path\", \"label\"]]\n",
    "print(f\"There are ** {len(df)} ** audio files in the dataset.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T19:04:06.533862Z",
     "start_time": "2023-10-19T19:03:55.229360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_clics_odontocetes\\.env\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "audio_util = AudioUtil()\n",
    "sig, sr = audio_util.open(df.loc[0, \"relative_path\"])\n",
    "spec = audio_util.mel_spectro_gram(sig, sr)\n",
    "spectrograme_shape = audio_util.get_audio_specs_size(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:36:44.948147Z",
     "start_time": "2023-10-19T19:06:06.741782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spectrograms generation...\n",
      "Spectrograms generated !0000/10000\n",
      "\n",
      "Saving spectrograms...\n",
      "Spectrograms saved !\n",
      "Global shape :  (10000, 128, 101)\n"
     ]
    }
   ],
   "source": [
    "def save_spectrograms(nb_files = len(df)):\n",
    "    audio_util = AudioUtil()\n",
    "    audio_specs = np.empty((0, spectrograme_shape[0], spectrograme_shape[1]))\n",
    "    label_files = np.empty(0)\n",
    "    print(\"Starting spectrograms generation...\")\n",
    "    for line_num in range(nb_files):\n",
    "        print(f\"Generating spectrogram {line_num+1}/{nb_files}\", end='\\r')\n",
    "        sig, sr = audio_util.open(df.loc[line_num, \"relative_path\"])\n",
    "        spec = audio_util.mel_spectro_gram(sig, sr)\n",
    "        audio_specs = np.append(audio_specs, [spec] , axis=0)\n",
    "        label_files = np.append(label_files, df.loc[line_num, \"label\"])\n",
    "    print(\"Spectrograms generated !\", end='\\n\\n')\n",
    "\n",
    "    print(\"Saving spectrograms...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"audio_specs.npy\"), audio_specs)\n",
    "    np.save(os.path.join(\"numpy_data\", \"label_files.npy\"), label_files)\n",
    "    print(\"Spectrograms saved !\")\n",
    "    print(\"Global shape : \", audio_specs.shape)\n",
    "\n",
    "save_spectrograms(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:37:49.516100Z",
     "start_time": "2023-10-19T20:37:48.929600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128, 101)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "def get_spectrograms_from_file(file_path):\n",
    "    audio_specs = np.load(file_path)\n",
    "    return audio_specs\n",
    "\n",
    "def get_labels_from_file(file_path):\n",
    "    label_files = np.load(file_path)\n",
    "    return label_files\n",
    "\n",
    "spectro = get_spectrograms_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"audio_specs.npy\"))\n",
    "labels = get_labels_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"label_files.npy\"))\n",
    "\n",
    "print(spectro.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:37:54.131688Z",
     "start_time": "2023-10-19T20:37:54.123689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectrograms for training : 8000\n",
      "Number of spectrograms for testing : 2000\n"
     ]
    }
   ],
   "source": [
    "size_train = int(0.8 * len(spectro))\n",
    "size_test = len(spectro) - size_train\n",
    "\n",
    "train_spectro = spectro[:size_train]\n",
    "train_labels = labels[:size_train]\n",
    "\n",
    "test_spectro = spectro[size_train:]\n",
    "test_labels = labels[size_train:]\n",
    "\n",
    "print(f\"Number of spectrograms for training : {len(train_spectro)}\")\n",
    "print(f\"Number of spectrograms for testing : {len(test_spectro)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:37:57.796700Z",
     "start_time": "2023-10-19T20:37:57.793987Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, spectro, labels, transform=None):\n",
    "        self.spectro = spectro\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectro)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        spectro = self.spectro[idx]\n",
    "        label = self.labels[idx].astype(np.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            spectro = self.transform(spectro)\n",
    "\n",
    "        return spectro, label\n",
    "\n",
    "training_dataset = MyDataset(train_spectro, train_labels)\n",
    "testing_dataset = MyDataset(test_spectro, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:38:00.159801Z",
     "start_time": "2023-10-19T20:38:00.156624Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = torch.utils.data.DataLoader(training_dataset, batch_size=128, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(testing_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T00:35:37.828449Z",
     "start_time": "2023-10-20T00:35:37.824950Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=256, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(3, 3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), padding=1)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        # self.fc1 = nn.Linear(64 * 31 * 2, 128)  \n",
    "        self.fc1 = nn.Linear(827392 , 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.Flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.Flatten(x)\n",
    "        # x = x.view(64 * 31 * 2)  # Adjust the input size based on your mel spectrogram size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:39:13.458264Z",
     "start_time": "2023-10-19T20:39:13.069265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioClassifier(\n",
      "  (conv1): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=827392, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model and put it on the GPU if available\n",
    "input_channels = 1\n",
    "num_classes = 2\n",
    "model = AudioClassifier(input_channels, num_classes).double() # .to(torch.float32)\n",
    "print(model)\n",
    "\n",
    "# device = \"cpu\"\n",
    "# Load model from pth file\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model = model.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:39:14.610327Z",
     "start_time": "2023-10-19T20:39:14.605828Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # binary crossentropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5240, Validation Accuracy: 4.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tristan Gonçalves\\Documents\\spe_IA_clics_odontocetes\\notebook_CRAP.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tristan%20Gon%C3%A7alves/Documents/spe_IA_clics_odontocetes/notebook_CRAP.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Zero the gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tristan%20Gon%C3%A7alves/Documents/spe_IA_clics_odontocetes/notebook_CRAP.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39mdouble()\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mto(device)  \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tristan%20Gon%C3%A7alves/Documents/spe_IA_clics_odontocetes/notebook_CRAP.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39;49mto(device))\u001b[39m.\u001b[39mto(device)  \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tristan%20Gon%C3%A7alves/Documents/spe_IA_clics_odontocetes/notebook_CRAP.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tristan%20Gon%C3%A7alves/Documents/spe_IA_clics_odontocetes/notebook_CRAP.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update weights\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "## Training loop\n",
    "\n",
    "num_epochs = 10\n",
    "train_loader = train_generator\n",
    "val_loader = val_generator\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(inputs.double().to(device)).to(device)  # Forward pass\n",
    "        loss = criterion(outputs.to(device), labels.to(device)).to(device)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Update the progress bar description with the current loss\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs.double().to(device))\n",
    "            val_loss += criterion(outputs.to(device), labels.to(device)).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.to(device)).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {100 * accuracy:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 128, 101])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for inputs, labels in train_loader:\n",
    "    # Check the shape of inputs\n",
    "    print(inputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
