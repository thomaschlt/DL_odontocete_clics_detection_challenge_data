{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div align=\"center\" >\n",
    "\n",
    "![Confusion Matrix](ENSC.png)\n",
    "\n",
    "# <u> ENSC Parcours IA </u>\n",
    "## Data Challenge - Détection de clics d'odontocètes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature as feat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import soundfile\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the downloaded data\n",
    "download_path = Path.cwd() / \".dataset\"\n",
    "\n",
    "# Read labels file\n",
    "labels_file = download_path / \"Y_train_ofTdMHi.csv\"\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df[\"relative_path\"] = str(download_path) + \"/X_train/\" + df[\"id\"]\n",
    "\n",
    "# Drop id column (replaced it with relative_path)\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "df.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "\n",
    "# invert relative_path and label columns positions\n",
    "df = df[[\"relative_path\", \"label\"]]\n",
    "display(ipd.Markdown(f\"### There are **_{len(df)}_** audio files in the dataset.\"))\n",
    "\n",
    "table = f\"\"\"\n",
    "Here is the split into good and bad signals:\n",
    "| Label   | Count   |\n",
    "|:-------:|:-------:|\n",
    "| 0       | {df['label'].value_counts()[0]} |\n",
    "| 1       | {df['label'].value_counts()[1]} |\"\"\"\n",
    "display(ipd.Markdown(table))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one file to check mfcc with librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file and load it\n",
    "file_path = df.loc[1, \"relative_path\"]\n",
    "audio_data, sr = librosa.load(file_path, sr=None)\n",
    "print(f\"Audio data shape: {audio_data.shape}, Sample rate: {sr}\")\n",
    "\n",
    "# Plot the audio signal\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "plt.plot(audio_data)\n",
    "plt.title('Waveform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil:\n",
    "    \"\"\"Static class for audio processing helper functions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(audio_file: str):\n",
    "        \"\"\"Load an audio file. Return the signal as a tensor and the sample rate\"\"\"\n",
    "        sig, sr = librosa.load(audio_file, sr=256000)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_duration(sig, sr):\n",
    "        \"\"\"Return the duration of an audio signal in seconds\"\"\"\n",
    "        return librosa.get_duration(sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectro_gram(sig: np.array, sr: int, n_mels=32, n_fft=1024):\n",
    "        \"\"\"Generate a Spectrogram\"\"\"\n",
    "        # get mel spectrogram\n",
    "        spec = librosa.feature.melspectrogram(y=sig, sr=sr)\n",
    "        spec = librosa.amplitude_to_db(spec)\n",
    "        return spec\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_mfccs(file_path):\n",
    "        audio_data, sr = librosa.load(file_path, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=25, n_mels=32)\n",
    "        mfccs_scaled_features = np.mean(mfccs.T,axis=0)\n",
    "        return mfccs_scaled_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_specs_size(spec):\n",
    "        \"\"\"Return the size of a spectrogram image\"\"\"\n",
    "        return spec.shape\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_mel_spectro_gram(spec: np.array, sr: int):\n",
    "        \"\"\"Plot a Spectrogram\"\"\"\n",
    "        # plot mel spectrogram\n",
    "        fig, ax = plt.subplots()\n",
    "        S_dB = librosa.power_to_db(spec, ref=np.max)\n",
    "        img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                                y_axis='mel', sr=sr,\n",
    "                                ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrograms(nb_files = len(df)):\n",
    "    audio_util = AudioUtil()\n",
    "    label_files = np.empty(0)\n",
    "    audio_mfccs = []\n",
    "    features_and_labels = []\n",
    "    print(\"Starting mfccs generation...\")\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Generating mfccs\"):\n",
    "        file_path = df.loc[line_num, \"relative_path\"]\n",
    "        mfccs = audio_util.extract_mfccs(file_path)\n",
    "        audio_mfccs.append(mfccs)\n",
    "        label_files = np.append(label_files, df.loc[line_num, \"label\"])\n",
    "        features_and_labels.append((mfccs, df.loc[line_num, \"label\"]))\n",
    "    print(\"Mfccs generated !\", end='\\n\\n')\n",
    "\n",
    "    print(\"Saving mfccs...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"audio_mfccs.npy\"), audio_mfccs)\n",
    "    np.save(os.path.join(\"numpy_data\", \"label_files.npy\"), label_files)\n",
    "    print(\"Mfccs saved !\")\n",
    "    features_and_labels = pd.DataFrame(features_and_labels, columns=[\"mfccs\", \"label\"])\n",
    "\n",
    "    print(\"Global shape : \", features_and_labels.shape)\n",
    "    print(features_and_labels.head())\n",
    "\n",
    "save_spectrograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file):\n",
    "    sig, sr = soundfile.read(audio_file)\n",
    "\n",
    "    sos = signal.butter(6, [5000, 100000], 'bandpass', fs=sr, output='sos')\n",
    "    sig = signal.sosfiltfilt(sos, sig)\n",
    "    rms = feat.rms(y=sig) \n",
    "    sc = feat.spectral_centroid(y=sig, sr=sr)\n",
    "    sb = feat.spectral_bandwidth(y=sig,sr=sr)\n",
    "    sf = feat.spectral_flatness(y=sig)\n",
    "\n",
    "    features = [np.mean(rms), np.std(rms), np.min(rms), np.max(rms),\\\n",
    "                np.mean(sc), np.std(sc), np.min(sc), np.max(sc),\\\n",
    "                np.mean(sb), np.std(sb), np.min(sb), np.max(sb),\\\n",
    "                np.mean(sf), np.std(sf), np.min(sf), np.max(sf)]\n",
    "    return features\n",
    "\n",
    "def save_features(nb_files = len(df)):\n",
    "    features_and_labels = []\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Generating features\"):\n",
    "        label_file = df.loc[line_num, \"label\"]\n",
    "        file_path = df.loc[line_num, \"relative_path\"]\n",
    "        features = extract_features(file_path)\n",
    "        features_and_labels.append((features, label_file))\n",
    "\n",
    "    features_and_labels = pd.DataFrame(features_and_labels, columns=[\"features\", \"label\"])\n",
    "    print(\"Global shape : \", features_and_labels.shape)\n",
    "    print(features_and_labels.head())\n",
    "\n",
    "    print(\"Saving features...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"features.npy\"), features_and_labels[\"features\"])\n",
    "    print(\"Features saved !\")\n",
    "\n",
    "# save_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrograms_from_file(file_path):\n",
    "    audio_specs = np.load(file_path)\n",
    "    return np.array(audio_specs.tolist())\n",
    "\n",
    "def get_labels_from_file(file_path):\n",
    "    label_files = np.load(file_path)\n",
    "    return np.array(label_files.tolist())\n",
    "\n",
    "def get_features_from_file(file_path):\n",
    "    features = np.load(file_path, allow_pickle=True)\n",
    "    return np.array(features.tolist())\n",
    "\n",
    "# X = get_features_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"features.npy\"))\n",
    "X = pd.DataFrame(get_spectrograms_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"audio_mfccs.npy\")))\n",
    "y = get_labels_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"label_files.npy\"))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# assert len(X[0]) == 16, \"Wrong number of features !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make features as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\"rms_mean\",\"rms_std\", \"rms_min\", \"rms_max\", \"sc_mean\", \"sc_std\", \"sc_min\", \"sc_max\", \"sb_mean\", \"sb_std\", \"sb_min\", \"sb_max\", \"sf_mean\", \"sf_std\", \"sf_min\", \"sf_max\"]\n",
    "X_with_features_columns = pd.DataFrame(columns=features)\n",
    "\n",
    "for i, X_element in tqdm(enumerate(X), unit=\" Element\", total=X.shape[0]):\n",
    "    for j, feature in enumerate(X_element):\n",
    "        X_with_features_columns.loc[i, features[j]] = feature\n",
    "X = X_with_features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, testing (and validation) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,train_size=0.8, random_state=64)\n",
    "# X_train, X_validation, y_train, y_validation=train_test_split(X_train,y_train,train_size=0.8, random_state=64)\n",
    "\n",
    "print(f\"X_train contains {X_train.shape[0]} files of shape {X_train.shape[1:]}\")\n",
    "print(f\"X_test contains {X_test.shape[0]} files of shape {X_test.shape[1:]}\", end='\\n\\n')\n",
    "# print(f\"X_validation contains {X_validation.shape[0]} files of shape {X_validation.shape[1:]}\")\n",
    "\n",
    "print(f\"Features are : {X_train.columns.tolist()}\") \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for prediction on random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def predict_on_random_sample(classifier, X_test, y_test):\n",
    "    random_index = random.randint(0, len(X_test)-1)\n",
    "    while random_index not in X_test.index:\n",
    "        random_index = random.randint(0, len(X_test)-1)\n",
    "    sample = X_test.loc[random_index]\n",
    "    print(\"Correct ✅\" if classifier.predict([sample])[0] == y_test[random_index] else \"Incorrect ❌\")\n",
    "    print(f'Prediction for sample {random_index:4}: {int(classifier.predict([sample])[0])}')\n",
    "    print(f'Actual label              : {int(y_test[random_index])}')\n",
    "\n",
    "    prob_table = f\"\"\"\n",
    "    Probabilities for each class:\n",
    "     _________________________\n",
    "    | Label   | Probability   |\n",
    "    |---------|---------------|\n",
    "    | 0       | {f\"{classifier.predict_proba([sample])[0][0]:.3f}\":13} |\n",
    "    | 1       | {f\"{classifier.predict_proba([sample])[0][1]:.3f}\":13} |\n",
    "    |_________|_______________|\"\"\"\n",
    "    display(ipd.Markdown(prob_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(classifier, X_test, y_test, cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    score_classifier = classifier.score(X_test, y_test)\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        display_labels=[0,1],\n",
    "        cmap=cmap,\n",
    "        normalize='true',\n",
    "    )\n",
    "    plt.ylabel('Actual label', fontsize = 17);\n",
    "    plt.xlabel('Predicted label', fontsize = 17);\n",
    "    plt.title(f'Accuracy Score: {score_classifier:.3f}', size = 17);\n",
    "    plt.tick_params(labelsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Logistic Regression ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(log_reg, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log_reg = log_reg.score(X_test_scaled, y_test)\n",
    "print(f\"Logistic Regression score: {score_log_reg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the confusion matrix on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(log_reg, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Logistic Regression : {roc_auc_score(y_test, y_pred_log_reg)}\")\n",
    "print(f\"F1 Score with Logistic Regression      : {f1_score(y_test, y_pred_log_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Decision Tree ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=0)\n",
    "tree_clf.fit(X_train.values, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {tree_clf.score(X_test.values, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(tree_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tree_clf = tree_clf.score(X_test.values, y_test)\n",
    "print(f\"Decision Tree score: {score_tree_clf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to find the optimal `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for max_depth:\n",
    "max_depth_range = list(range(1, 15))\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "accuracy = []\n",
    "\n",
    "for depth in tqdm(max_depth_range, unit='depth', desc='Testing max_depth'):\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth = depth, \n",
    "                             random_state = 0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "\n",
    "ax.plot(max_depth_range,\n",
    "        accuracy,\n",
    "        lw=2,\n",
    "        color='k')\n",
    "\n",
    "ax.set_xlim([1, len(max_depth_range)])\n",
    "ax.set_ylim([.50, 1.00])\n",
    "ax.grid(True,\n",
    "        axis = 'both',\n",
    "        zorder = 0,\n",
    "        linestyle = ':',\n",
    "        color = 'k')\n",
    "\n",
    "yticks = ax.get_yticks()\n",
    "\n",
    "y_ticklist = []\n",
    "for tick in yticks:\n",
    "    y_ticklist.append(str(tick).ljust(4, '0')[0:4])\n",
    "ax.set_yticklabels(y_ticklist)\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('max_depth', fontsize = 24)\n",
    "ax.set_ylabel('Accuracy', fontsize = 24)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# Caution on the max_depth parameter, it makes the tree unreadable if too big\n",
    "# tree.plot_tree(tree_clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree_clf, X_test.values, y_test, cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Decision Tree : {roc_auc_score(y_test, y_pred_tree):.3f}\")\n",
    "print(f\"F1 Score with Decision Tree      : {f1_score(y_test, y_pred_tree):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Bagged Tree ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier()\n",
    "\n",
    "bag_clf.fit(X_train.values, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {bag_clf.score(X_test.values, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(bag_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_bag = bag_clf.score(X_test.values, y_test)\n",
    "print(f\"Logistic Regression score: {score_bag:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(bag_clf, X_test.values, y_test, cmap=plt.cm.Purples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Bagged Tree : {roc_auc_score(y_test, y_pred_bag):.3f}\")\n",
    "print(f\"F1 Score with Bagged Tree      : {f1_score(y_test, y_pred_bag):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Random Forest ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=500)\n",
    "forest_clf.fit(X_train.values, y_train)\n",
    "y_pred_forest = forest_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_forest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try predicting on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(forest_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_forest = forest_clf.score(X_test.values, y_test)\n",
    "print(f\"Logistic Regression score: {score_forest:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(forest_clf, X_test.values, y_test, cmap=plt.cm.Oranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Random Forest : {roc_auc_score(y_test, y_pred_forest):.3f}\")\n",
    "print(f\"F1 Score with Random Forest      : {f1_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the importance of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(forest_clf.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False)\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of previous models and their respective scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.get_params()\n",
    "type(log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending by score\n",
    "classifiers_names = [\"Logistic Regression\", \"Decision Tree\", \"Bagging\", \"Random Forest\"]\n",
    "classifiers_scores = [score_log_reg, score_tree_clf, score_bag, score_forest]\n",
    "\n",
    "classifiers_and_scores = list(zip(classifiers_names, classifiers_scores))\n",
    "classifiers_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "scores_lines = [f\"| **{name}** | **{score:.3f}** |\\n\" for index, (name, score) in enumerate(classifiers_and_scores)]\n",
    "\n",
    "scores_table = f\"\"\"\n",
    "<div align=\"center\">\n",
    "    <h2>Classifiers scores</h2>\n",
    "\n",
    "| <h3>Classifier</h3>   | <h3>Score</h3>   |\n",
    "|:---------:|:---------------:|\n",
    "{\"\".join(scores_lines)}\n",
    "\"\"\"\n",
    "\n",
    "display(ipd.Markdown(scores_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classifier seems to be the random forest, thus we will make predictions on the test waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd() / \".dataset\"\n",
    "\n",
    "# Read metadata file\n",
    "labels_file = download_path / \"Y_random_Xwjr6aB.csv\"\n",
    "df_test = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df_test[\"relative_path\"] = str(download_path) + \"/X_test/\" + df_test[\"id\"]\n",
    "df_test.drop(columns=[\"id\"], inplace=True)\n",
    "df_test.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "# invert relative_path and label columns positions\n",
    "df_test = df_test[[\"relative_path\", \"label\"]]\n",
    "print(f\"There are ** {len(df_test)} ** audio files in the test dataset.\", end=\"\\n\\n\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "audio_util = AudioUtil()\n",
    "predictions = []\n",
    "\n",
    "for line_num in tqdm(range(len(df_test)), unit=\"file\", desc=\"Predicting labels\"):\n",
    "    test_audio_file = df_test.loc[line_num, \"relative_path\"]\n",
    "    test_file_features = np.array(audio_util.extract_mfccs(test_audio_file))\n",
    "    \n",
    "    # Predict the label of the audio file\n",
    "    prediction = forest_clf.predict(test_file_features.reshape(1, -1))\n",
    "\n",
    "    # Print the predicted label\n",
    "    if prediction > 0.5:\n",
    "        predictions.append([df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], 1])\n",
    "    else:\n",
    "        predictions.append([df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], 0])\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns=[\"id\", \"pos_label\"])\n",
    "\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "predictions.to_csv(f\"Y_predict_{date}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_not_Cnn():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(256, input_shape=(40,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    model.summary()\n",
    "   \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Reshape((40, 1), input_shape=(40,)))\n",
    "\n",
    "    # Add a Lambda layer to add the batch size and channel dimensions\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1)))\n",
    "\n",
    "    # Add the first convolutional layer\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 1), activation='relu', strides=(1, 1))) \n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    # Add the second convolutional layer\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 1), activation='relu', strides=(1, 1), padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "is_cnn = \"CNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class Reshape(nn.Module):\n",
    "#     def __init__(self, shape):\n",
    "#         super(Reshape, self).__init__()\n",
    "#         self.shape = shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(x.size(0), *self.shape)\n",
    "\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomModel, self).__init__()\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             Reshape((1, 40)),\n",
    "#             nn.Conv2d(1, 256, kernel_size=(3, 1), stride=(1, 1)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Conv2d(256, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(1280, 256),  # Adjust the input size based on the output size of the previous layer\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# def build_model():\n",
    "#     return CustomModel()\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = build_model()\n",
    "\n",
    "# # Print the model summary\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 200\n",
    "num_batch_size = 32\n",
    "\n",
    "# Create an EarlyStopping callback montinotoring 'val_loss'\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_validation, y_validation), verbose=1, callbacks=[early_stopping_callback]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = model.evaluate(X_test,y_test,verbose=0)\n",
    "\n",
    "for metric_name, result in zip(model.metrics_names, evaluation_results):\n",
    "    print(f\"{metric_name}: {result:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model depending on date and accuracy\n",
    "# Get date\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model.save(os.path.join(os.getcwd(),\"models\",f\"model_{date}_{evaluation_results[1]:.4f}_{is_cnn}.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from tensorflow.keras.models import load_model\n",
    "options = [model for model in os.listdir(os.path.join(os.getcwd(),\"models\")) if model.endswith(\".h5\")]\n",
    "dropdown = widgets.Dropdown(options=options, value=options[0], description='Select a model:')\n",
    "button = widgets.Button(description='Show selected model')\n",
    "\n",
    "output = widgets.Output()\n",
    "selected_model = model\n",
    "def on_button_click(b):\n",
    "    global selected_model\n",
    "    with output:\n",
    "        selected_model = load_model(os.path.join(os.getcwd(),\"models\",dropdown.value))\n",
    "        print(f\"Selected model: {dropdown.value}\")\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "display(dropdown)\n",
    "display(button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd() / \".dataset\"\n",
    "\n",
    "# Read metadata file\n",
    "labels_file = download_path / \"Y_random_Xwjr6aB.csv\"\n",
    "df_test = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df_test[\"relative_path\"] = str(download_path) + \"/X_test/\" + df_test[\"id\"]\n",
    "df_test.drop(columns=[\"id\"], inplace=True)\n",
    "df_test.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "# invert relative_path and label columns positions\n",
    "df_test = df_test[[\"relative_path\", \"label\"]]\n",
    "print(f\"There are ** {len(df_test)} ** audio files in the test dataset.\", end=\"\\n\\n\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrograms_test(nb_files = len(df_test)):\n",
    "    audio_util = AudioUtil()\n",
    "    labels_predicted = np.empty(0)\n",
    "    files_with_labels = []\n",
    "    audio_mfccs = []\n",
    "    labels_files = np.empty(0)\n",
    "    features_and_labels = []\n",
    "    print(\"Starting mfccs generation...\")\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Generating mfccs\"):\n",
    "        file_path = df_test.loc[line_num, \"relative_path\"]\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        mfccs = audio_util.extract_mfccs(file_path)\n",
    "        \n",
    "        audio_mfccs.append(mfccs)\n",
    "        labels_files = np.append(labels_files, df_test.loc[line_num, \"label\"])\n",
    "\n",
    "        single_prediction = model.predict(mfccs.reshape(1,40), verbose=0)\n",
    "        labels_predicted = np.append(labels_predicted, single_prediction[0][0])\n",
    "        features_and_labels.append((mfccs, df_test.loc[line_num, \"label\"]))\n",
    "\n",
    "        files_with_labels.append([file_name, labels_predicted[-1]])\n",
    "    print(\"Mfccs generated !\", end='\\n\\n')\n",
    "    \n",
    "    files_with_labels = pd.DataFrame(files_with_labels, columns=[\"id\", \"label\"])\n",
    "    print(files_with_labels.head())\n",
    "    \n",
    "    # Save the csv file with the predictions\n",
    "    files_with_labels.to_csv(\"Y_test_predictions.csv\", index=False)\n",
    "\n",
    "    print(\"Saving mfccs...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"test_audio_mfccs.npy\"), audio_mfccs)\n",
    "    np.save(os.path.join(\"numpy_data\", \"test_label_files.npy\"), labels_files)\n",
    "    print(\"Mfccs saved !\")\n",
    "    \n",
    "    features_and_labels = pd.DataFrame(features_and_labels, columns=[\"mfccs\", \"label\"])\n",
    "\n",
    "    print(\"Global shape : \", features_and_labels.shape)\n",
    "    print(features_and_labels.head())\n",
    "\n",
    "# save_spectrograms_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the AudioUtil class\n",
    "test_audio_util = AudioUtil()\n",
    "predictions = []\n",
    "\n",
    "for line_num in tqdm(range(len(df_test)), unit=\"file\", desc=\"Predicting labels\"):\n",
    "    test_audio_file = df_test.loc[line_num, \"relative_path\"]\n",
    "    test_mfccs = test_audio_util.extract_mfccs(test_audio_file)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(f\"models/model_31-10-2023_14-39-36_0.9668_CNN.h5\")\n",
    "\n",
    "    # Predict the label of the audio file\n",
    "    prediction = model.predict(test_mfccs.reshape(1,40), verbose=0);\n",
    "\n",
    "    # Print the predicted label\n",
    "    if prediction > 0.5:\n",
    "        predictions.append([df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], 1])\n",
    "    else:\n",
    "        predictions.append([df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for prediction in predictions:\n",
    "    if prediction[1] == 1:\n",
    "        print(prediction, i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the ROC AUC score with sklearn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "X_testing = get_spectrograms_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"test_audio_mfccs.npy\"))\n",
    "# y_testing = np.load(os.path.join(os.getcwd(),\"numpy_data\", \"test_label_files.npy\"))\n",
    "y_pred_keras = model.predict(X_test).ravel()\n",
    "y_pred_keras = np.where(y_pred_keras > 0.5, 1, 0)\n",
    "print(y_pred_keras, end=\"\\n\\n\")\n",
    "print(roc_auc_score(y_test, y_pred_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the F1 score with sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the csv file with the predictions and the file names\n",
    "# predictions = [[df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], y_pred_keras[i]] for i, line_num in enumerate(range(len(df_test)))]\n",
    "df_predictions = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\n",
    "df_predictions.to_csv(\"Y_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
