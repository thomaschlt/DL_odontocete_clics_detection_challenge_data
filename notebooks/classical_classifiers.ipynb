{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div align=\"center\" >\n",
    "\n",
    "![Confusion Matrix](../images/ENSC.png)\n",
    "\n",
    "# <u> ENSC Parcours IA </u>\n",
    "## Data Challenge - Détection de clics d'odontocètes\n",
    "\n",
    "</div>\n",
    "\n",
    "As part of the [Artificial Intelligence specialization](https://3aia.notion.site/3aia/Parcours-3A-IA-2023-9917027c682b457dae71fea68c067ad1) at the [ENSC](https://ensc.bordeaux-inp.fr/fr), we participated in a data challenge provided by the University of Toulon in the [ChallengeData](https://challengedata.ens.fr/) website. \n",
    "\n",
    "This challenge specifically aims to detect the presence of odontoceti clicks in underwater audio recordings in the Caribbean sea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature as feat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import soundfile\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the downloaded data\n",
    "download_path = Path.cwd() / \"../\" / \".dataset\"\n",
    "\n",
    "# Read labels file\n",
    "labels_file = download_path / \"Y_train_ofTdMHi.csv\"\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df[\"relative_path\"] = str(download_path) + \"/X_train/\" + df[\"id\"]\n",
    "\n",
    "# Drop id column (replaced it with relative_path)\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "df.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "\n",
    "# invert relative_path and label columns positions\n",
    "df = df[[\"relative_path\", \"label\"]]\n",
    "display(ipd.Markdown(f\"### There are **_{len(df)}_** audio files in the dataset.\"))\n",
    "\n",
    "table = f\"\"\"\n",
    "Here is the split into good and bad signals:\n",
    "| Label   | Count   |\n",
    "|:-------:|:-------:|\n",
    "| 0       | {df['label'].value_counts()[0]} |\n",
    "| 1       | {df['label'].value_counts()[1]} |\"\"\"\n",
    "display(ipd.Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one file to check mfcc with librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file and load it\n",
    "file_paths = [df.loc[0, \"relative_path\"], df.loc[1, \"relative_path\"]]\n",
    "file_labels= [df.loc[0, \"label\"], df.loc[1, \"label\"]]\n",
    "audio_data1, sr1 = librosa.load(file_paths[0], sr=None)\n",
    "audio_data2, sr2 = librosa.load(file_paths[1], sr=None)\n",
    "print(f\"Audio data shape: {audio_data1.shape}, Sample rate: {sr1}\")\n",
    "\n",
    "# Plot the audio signal\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n",
    "fig.suptitle('Waveforms for good and wrong signal', fontsize=16)\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"Time\")\n",
    "    ax[i].set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax[0].plot(audio_data1)\n",
    "ax[0].title.set_text('Waveform for wrong signal')\n",
    "\n",
    "ax[1].plot(audio_data2)\n",
    "ax[1].title.set_text('Waveform for good signal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil:\n",
    "    \"\"\"Static class for audio processing helper functions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(audio_file: str):\n",
    "        \"\"\"Load an audio file. Return the signal as a tensor and the sample rate\"\"\"\n",
    "        sig, sr = librosa.load(audio_file, sr=256000)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_duration(sig, sr):\n",
    "        \"\"\"Return the duration of an audio signal in seconds\"\"\"\n",
    "        return librosa.get_duration(sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectro_gram(sig: np.array, sr: int, n_mels=32, n_fft=1024):\n",
    "        \"\"\"Generate a Spectrogram\"\"\"\n",
    "        # get mel spectrogram\n",
    "        spec = librosa.feature.melspectrogram(y=sig, sr=sr)\n",
    "        spec = librosa.amplitude_to_db(spec)\n",
    "        return spec\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_mfccs(file_path):\n",
    "        audio_data, sr = librosa.load(file_path, sr = None)\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)\n",
    "        mfccs_scaled_features = np.mean(mfccs.T,axis=0)\n",
    "        return mfccs_scaled_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_specs_size(spec):\n",
    "        \"\"\"Return the size of a spectrogram image\"\"\"\n",
    "        return spec.shape\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_mel_spectro_gram(spec: np.array, sr: int):\n",
    "        \"\"\"Plot a Spectrogram\"\"\"\n",
    "        # plot mel spectrogram\n",
    "        fig, ax = plt.subplots()\n",
    "        S_dB = librosa.power_to_db(spec, ref=np.max)\n",
    "        img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                                y_axis='mel', sr=sr,\n",
    "                                ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfccs(nb_files = len(df)):\n",
    "    audio_util = AudioUtil()\n",
    "    label_files = np.empty(0)\n",
    "    audio_mfccs = []\n",
    "    features_and_labels = []\n",
    "    print(\"Starting mfccs generation...\")\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Generating mfccs\"):\n",
    "        file_path = df.loc[line_num, \"relative_path\"]\n",
    "        mfccs = audio_util.extract_mfccs(file_path) # , n_mels = \n",
    "        audio_mfccs.append(mfccs)\n",
    "        label_files = np.append(label_files, int(df.loc[line_num, \"label\"]))\n",
    "        features_and_labels.append((mfccs, df.loc[line_num, \"label\"]))\n",
    "    print(\"Mfccs generated !\", end='\\n\\n')\n",
    "\n",
    "    print(\"Saving mfccs...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"audio_mfccs.npy\"), audio_mfccs)\n",
    "    np.save(os.path.join(\"numpy_data\", \"label_files.npy\"), label_files)\n",
    "    print(\"Mfccs saved !\")\n",
    "    features_and_labels = pd.DataFrame(features_and_labels, columns=[\"mfccs\", \"label\"])\n",
    "\n",
    "    print(\"Global shape : \", features_and_labels.shape)\n",
    "    print(features_and_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mfccs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels(nb_files = len(df)):\n",
    "    label_files = np.empty(0)\n",
    "    print(\"Saving labels...\")\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Retrieving labels\"):\n",
    "        label_files = np.append(label_files, int(df.loc[line_num, \"label\"]))\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"label_files.npy\"), label_files)\n",
    "    print(\"Labels saved !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file):\n",
    "    sig, sr = soundfile.read(audio_file)\n",
    "\n",
    "    sos = signal.butter(6, [5000, 100000], 'bandpass', fs=sr, output='sos')\n",
    "    sig = signal.sosfiltfilt(sos, sig)\n",
    "    rms = feat.rms(y=sig) \n",
    "    sc = feat.spectral_centroid(y=sig, sr=sr)\n",
    "    sb = feat.spectral_bandwidth(y=sig,sr=sr)\n",
    "    sf = feat.spectral_flatness(y=sig)\n",
    "\n",
    "    features = [np.mean(rms), np.std(rms), np.min(rms), np.max(rms),\\\n",
    "                np.mean(sc), np.std(sc), np.min(sc), np.max(sc),\\\n",
    "                np.mean(sb), np.std(sb), np.min(sb), np.max(sb),\\\n",
    "                np.mean(sf), np.std(sf), np.min(sf), np.max(sf)]\n",
    "    return features\n",
    "\n",
    "def save_features(nb_files = len(df)):\n",
    "    features_and_labels = []\n",
    "    for line_num in tqdm(range(nb_files), unit=\"file\", desc=\"Generating features\"):\n",
    "        label_file = df.loc[line_num, \"label\"]\n",
    "        file_path = df.loc[line_num, \"relative_path\"]\n",
    "        features = extract_features(file_path)\n",
    "        features_and_labels.append((features, label_file))\n",
    "\n",
    "    features_and_labels = pd.DataFrame(features_and_labels, columns=[\"features\", \"label\"])\n",
    "    print(\"Global shape : \", features_and_labels.shape)\n",
    "    print(features_and_labels.head())\n",
    "\n",
    "    print(\"Saving features...\")\n",
    "    os.mkdir(\"numpy_data\") if not os.path.exists(\"numpy_data\") else None\n",
    "    np.save(os.path.join(\"numpy_data\", \"features.npy\"), features_and_labels[\"features\"])\n",
    "    print(\"Features saved !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfccs_from_file(file_path):\n",
    "    audio_specs = np.load(file_path)\n",
    "    return np.array(audio_specs.tolist())\n",
    "\n",
    "def get_labels_from_file(file_path):\n",
    "    label_files = np.load(file_path)\n",
    "    return np.array(label_files.tolist())\n",
    "\n",
    "def get_features_from_file(file_path):\n",
    "    features = np.load(file_path, allow_pickle=True)\n",
    "    return np.array(features.tolist())\n",
    "\n",
    "X = get_features_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"features.npy\"))\n",
    "# X = pd.DataFrame(get_mfccs_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"audio_mfccs.npy\")))\n",
    "y = get_labels_from_file(os.path.join(os.getcwd(),\"numpy_data\", \"label_files.npy\"))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# assert len(X[0]) == 16, \"Wrong number of features !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make features as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\"rms_mean\",\"rms_std\", \"rms_min\", \"rms_max\", \"sc_mean\", \"sc_std\", \"sc_min\", \"sc_max\", \"sb_mean\", \"sb_std\", \"sb_min\", \"sb_max\", \"sf_mean\", \"sf_std\", \"sf_min\", \"sf_max\"]\n",
    "X_with_features_columns = pd.DataFrame(columns=features)\n",
    "\n",
    "for i, X_element in tqdm(enumerate(X), unit=\" Element\", total=X.shape[0]):\n",
    "    for j, feature in enumerate(X_element):\n",
    "        X_with_features_columns.loc[i, features[j]] = feature\n",
    "X = X_with_features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, testing (and validation) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,train_size=0.75)\n",
    "# X_train, X_validation, y_train, y_validation=train_test_split(X_train,y_train,train_size=0.8, random_state=64)\n",
    "\n",
    "print(f\"X_train contains {X_train.shape[0]} files of shape {X_train.shape[1:]}\")\n",
    "print(f\"X_test contains {X_test.shape[0]} files of shape {X_test.shape[1:]}\", end='\\n\\n')\n",
    "# print(f\"X_validation contains {X_validation.shape[0]} files of shape {X_validation.shape[1:]}\")\n",
    "\n",
    "# print(f\"Features are : {X_train.columns.tolist()}\") \n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_fold = 2\n",
    "model = LogisticRegression()\n",
    "accuracy_scores = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "skf = StratifiedKFold(n_splits=n_fold)\n",
    "\n",
    "for train, test in skf.split(X_scaled, y):\n",
    "    X_train.append(train), X_test.append(test), y_train.append(train), y_test.append(test)\n",
    "\n",
    "print(\"N° of folds: \", len(X_train))\n",
    "print(\"N° of audios per fold: \",X_train[0].shape[0])\n",
    "\n",
    "for i in tqdm(range(n_fold), unit=\" fold\", desc=\"Training\"):\n",
    "    model.fit(X_train[i].reshape(-1,1), y_train[i])\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(X_test[i].reshape(-1, 1))\n",
    "    \n",
    "    # Calculate the accuracy score for this fold\n",
    "    acc = accuracy_score(y_test[i], y_pred)\n",
    "    \n",
    "    # Append the accuracy score to the list\n",
    "    accuracy_scores.append(acc)\n",
    "\n",
    "print(\"Accuracy scores: \", accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for prediction on random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def predict_on_random_sample(classifier, X_test, y_test):\n",
    "    random_index = random.randint(0, len(X_test)-1)\n",
    "    while random_index not in X_test.index:\n",
    "        random_index = random.randint(0, len(X_test)-1)\n",
    "    sample = X_test.loc[random_index]\n",
    "    print(\"Correct ✅\" if classifier.predict([sample])[0] == y_test[random_index] else \"Incorrect ❌\")\n",
    "    print(f'Prediction for sample {random_index:4}: {int(classifier.predict([sample])[0])}')\n",
    "    print(f'Actual label              : {int(y_test[random_index])}')\n",
    "\n",
    "    prob_table = f\"\"\"\n",
    "    Probabilities for each class:\n",
    "     _________________________\n",
    "    | Label   | Probability   |\n",
    "    |---------|---------------|\n",
    "    | 0       | {f\"{classifier.predict_proba([sample])[0][0]:.3f}\":13} |\n",
    "    | 1       | {f\"{classifier.predict_proba([sample])[0][1]:.3f}\":13} |\n",
    "    |_________|_______________|\"\"\"\n",
    "    display(ipd.Markdown(prob_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to display the confusion matrixes after fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(classifier, X_test, y_test, cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    score_classifier = classifier.score(X_test, y_test)\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        display_labels=[0,1],\n",
    "        cmap=cmap,\n",
    "        normalize='true',\n",
    "    )\n",
    "    plt.ylabel('Actual label', fontsize = 17);\n",
    "    plt.xlabel('Predicted label', fontsize = 17);\n",
    "    plt.title(f'Accuracy Score: {score_classifier:.3f}', size = 17);\n",
    "    plt.tick_params(labelsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data\n",
    "\n",
    "For some models (e.g. Logistic Regression), data needs to be scaled. In order to do that we will use scikit-learn's StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Logistic Regression ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(log_reg, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log_reg = log_reg.score(X_test_scaled, y_test)\n",
    "print(f\"Logistic Regression score: {score_log_reg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the confusion matrix on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(log_reg, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Logistic Regression : {roc_auc_score(y_test, y_pred_log_reg)}\")\n",
    "print(f\"F1 Score with Logistic Regression      : {f1_score(y_test, y_pred_log_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Decision Tree ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=0)\n",
    "tree_clf.fit(X_train.values, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {tree_clf.score(X_test.values, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(tree_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tree_clf = tree_clf.score(X_test.values, y_test)\n",
    "print(f\"Decision Tree score: {score_tree_clf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to find the optimal `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for max_depth:\n",
    "max_depth_range = list(range(1, 20))\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "accuracy = []\n",
    "\n",
    "for depth in tqdm(max_depth_range, unit='depth', desc='Testing max_depth'):\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth = depth, \n",
    "                             random_state = 0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "\n",
    "ax.plot(max_depth_range,\n",
    "        accuracy,\n",
    "        lw=2,\n",
    "        color='k')\n",
    "\n",
    "ax.set_xlim([1, len(max_depth_range)])\n",
    "ax.set_ylim([.50, 1.00])\n",
    "ax.grid(True,\n",
    "        axis = 'both',\n",
    "        zorder = 0,\n",
    "        linestyle = ':',\n",
    "        color = 'k')\n",
    "\n",
    "yticks = ax.get_yticks()\n",
    "\n",
    "y_ticklist = []\n",
    "for tick in yticks:\n",
    "    y_ticklist.append(str(tick).ljust(4, '0')[0:4])\n",
    "ax.set_yticklabels(y_ticklist)\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('max_depth', fontsize = 24)\n",
    "ax.set_ylabel('Accuracy', fontsize = 24)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# Caution on the max_depth parameter, it makes the tree unreadable if too big\n",
    "# tree.plot_tree(tree_clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree_clf, X_test.values, y_test, cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Decision Tree : {roc_auc_score(y_test, y_pred_tree):.3f}\")\n",
    "print(f\"F1 Score with Decision Tree      : {f1_score(y_test, y_pred_tree):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Bagged Tree ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier()\n",
    "\n",
    "bag_clf.fit(X_train.values, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {bag_clf.score(X_test.values, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try prediction on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(bag_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_bag = bag_clf.score(X_test.values, y_test)\n",
    "print(f\"Bagged tree score: {score_bag:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(bag_clf, X_test.values, y_test, cmap=plt.cm.Purples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Bagged Tree : {roc_auc_score(y_test, y_pred_bag):.3f}\")\n",
    "print(f\"F1 Score with Bagged Tree      : {f1_score(y_test, y_pred_bag):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- Random Forest ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "forest_clf.fit(X_train.values, y_train)\n",
    "y_pred_forest = forest_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_forest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try predicting on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(forest_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_forest = forest_clf.score(X_test.values, y_test)\n",
    "print(f\"Random forest score: {score_forest:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(forest_clf, X_test.values, y_test, cmap=plt.cm.Oranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with Random Forest : {roc_auc_score(y_test, y_pred_forest):.3f}\")\n",
    "print(f\"F1 Score with Random Forest      : {f1_score(y_test, y_pred_forest):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the importance of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(forest_clf.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False)\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> ---------- XGBClassifier ---------- </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train.values, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test.values)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_random_sample(xgb_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_xgb = xgb_clf.score(X_test.values, y_test)\n",
    "print(f\"XGB score: {score_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_clf, X_test.values, y_test, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ROC AUC & F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC Score with XGB : {roc_auc_score(y_test, y_pred_xgb):.3f}\")\n",
    "print(f\"F1 Score with XGB      : {f1_score(y_test, y_pred_xgb):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "scores = cross_val_score(xgb_clf, X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "kf_cv_scores = cross_val_score(xgb_clf, X_train, y_train, cv=kfold )\n",
    "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of previous models and their respective scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.get_params()\n",
    "type(log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending by score\n",
    "classifiers_names = [\"Logistic Regression\", \"Decision Tree\", \"Bagging\", \"Random Forest\", \"XGBClassifier\"]\n",
    "classifiers_scores = [score_log_reg, score_tree_clf, score_bag, score_forest, score_xgb]\n",
    "\n",
    "classifiers_f1_scores = [f1_score(y_test, y_pred_log_reg), f1_score(y_test, y_pred_tree), f1_score(y_test, y_pred_bag), f1_score(y_test, y_pred_forest), f1_score(y_test, y_pred_xgb)]\n",
    "\n",
    "classifiers_and_scores = list(zip(classifiers_names, classifiers_scores, classifiers_f1_scores))\n",
    "classifiers_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "scores_lines = [f\"| **{name}** | **{score:.3f}** | **{f1:.3f}** |\\n\" for index, (name, score, f1) in enumerate(classifiers_and_scores)]\n",
    "\n",
    "scores_table = f\"\"\"\n",
    "<div align=\"center\">\n",
    "    Classifiers scores\n",
    "\n",
    "| Classifier   | Score  | F1 Score   |\n",
    "\n",
    "{\"\".join(scores_lines)}\n",
    "\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(ipd.Markdown(scores_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classifier seems to be the XGB classifier, thus we will make predictions on the test waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd() / \"..\" / \".dataset\"\n",
    "\n",
    "# Read metadata file\n",
    "labels_file = download_path / \"Y_random_Xwjr6aB.csv\"\n",
    "df_test = pd.read_csv(labels_file)\n",
    "\n",
    "# Construct file path by concatenating folder and file name\n",
    "df_test[\"relative_path\"] = str(download_path) + \"/X_test/\" + df_test[\"id\"]\n",
    "df_test.drop(columns=[\"id\"], inplace=True)\n",
    "df_test.rename(columns={\"pos_label\": \"label\"}, inplace=True)\n",
    "# invert relative_path and label columns positions\n",
    "df_test = df_test[[\"relative_path\", \"label\"]]\n",
    "print(f\"There are ** {len(df_test)} ** audio files in the test dataset.\", end=\"\\n\\n\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "audio_util = AudioUtil()\n",
    "predictions = []\n",
    "\n",
    "for line_num in tqdm(range(len(df_test)), unit=\"file\", desc=\"Predicting labels\"):\n",
    "    test_audio_file = df_test.loc[line_num, \"relative_path\"]\n",
    "    test_file_features = np.array(extract_features(test_audio_file))\n",
    "    \n",
    "    # Predict the label of the audio file\n",
    "    prediction = xgb_clf.predict_proba(test_file_features.reshape(1, -1))\n",
    "\n",
    "    predictions.append([df_test.loc[line_num, \"relative_path\"].split(sep='/')[-1], max(prediction[0][0], prediction[0][1])])\n",
    "\n",
    "    \n",
    "predictions = pd.DataFrame(predictions, columns=[\"id\", \"pos_label\"])\n",
    "print(predictions.head())\n",
    "\n",
    "now = datetime.now()\n",
    "date = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "predictions.to_csv(f\"Y_predict_{date}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
